Feature Selection with Filtering Method- Constant, Quasi Constant and Duplicate Feature Removal
Download Working Files: https://github.com/laxmimerit/Feature-Selection-with-Filtering-Method--Constant-Quasi-Constant-and-Duplicate-Feature-Removal

Filters methods belong to the category of feature selection methods that select features independently of the machine learning algorithm model. This is one of the biggest advantages of filter methods. Features selected using filter methods can be used as an input to any machine learning models. Another advantage of filter methods is that they are very fast. Filter methods are generally the first step in any feature selection pipeline.

If your dataset has perfectly positive or negative attributes then there is a high chance that the performance of the model will be impacted by a problem called — “Multicollinearity”. Multicollinearity happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results. Luckily, decision trees and boosted trees algorithms are immune to multicollinearity by nature. When they decide to split, the tree will choose only one of the perfectly correlated features. However, other algorithms like Logistic Regression or Linear Regression are not immune to that problem and you should fix it before training the model.

Like Facebook Page: https://www.facebook.com/kgptalkie/

Watch Full Playlists:
Feature Selection in Machine Learning using Python: https://www.youtube.com/playlist?list=PLc2rvfiptPSQYzmDIFuq2PqN2n28ZjxDH

Machine Learning with Theory and Example https://www.youtube.com/playlist?list=PLc2rvfiptPSTvPFbNlT_TGRupzKKhJSIv

Make Your Own Automated Email Marketing Software in Python: https://www.youtube.com/watch?v=gmYuom6kfoY&list=PLc2rvfiptPSQK9ErKaLqf40iu1A3le9Zr
